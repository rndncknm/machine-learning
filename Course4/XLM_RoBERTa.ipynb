{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "XLM-RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USzDlRSI-aaV",
        "colab_type": "text"
      },
      "source": [
        "## XLM-RoBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNl2mORH-aaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS_ruXtb-aab",
        "colab_type": "text"
      },
      "source": [
        "Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlNWxHGE-aaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_attention_masks=False, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmb0XpVE-aaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(transformer, loss='binary_crossentropy', max_len=512):\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss=loss, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK0ggYtOpvGH",
        "colab_type": "text"
      },
      "source": [
        "Focal loss (давала примерно такие же результаты, как и binary crossentropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcDAkW6xpgQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def focal_loss(gamma=2., alpha=.2):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
        "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
        "        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n",
        "    return focal_loss_fixed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xr95ygo-aam",
        "colab_type": "text"
      },
      "source": [
        "ТПУ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhxQ1Hyj-aan",
        "colab_type": "code",
        "outputId": "96e778ce-8b5b-4d70-9bd1-494e6996f0d5",
        "colab": {}
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  grpc://10.0.0.2:8470\n",
            "REPLICAS:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loonUAF8-zy6",
        "colab_type": "text"
      },
      "source": [
        "Проводились эксперименты с BATCH_SIZE, MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YA-UDeb-aar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "MAX_LEN = 216\n",
        "MODEL = 'jplu/tf-xlm-roberta-large'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOiTC4rU-aau",
        "colab_type": "text"
      },
      "source": [
        "Токенайзер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-0tX6np-aav",
        "colab_type": "code",
        "outputId": "dddfcdf5-3b83-4a6f-98fd-ac34a754ba79",
        "colab": {
          "referenced_widgets": [
            "04bb24e46862482d8684e379aa7058a2",
            "bd8b1fafc35b427799541450f816d0a5"
          ]
        }
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04bb24e46862482d8684e379aa7058a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd8b1fafc35b427799541450f816d0a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Es24uQC-aa0",
        "colab_type": "text"
      },
      "source": [
        "Данные и обработка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4ynk_JU-aa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
        "train2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\n",
        "train2.toxic = train2.toxic.round().astype(int)\n",
        "\n",
        "valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
        "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
        "sub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fffGJufb-aa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.concat([\n",
        "    train1[['comment_text', 'toxic']],\n",
        "    train2[['comment_text', 'toxic']].query('toxic==1'),\n",
        "    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=100000, random_state=0)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Y1J3t2-aa7",
        "colab_type": "code",
        "outputId": "8eabf9b5-890a-4734-8c19-233c54e7c090",
        "colab": {}
      },
      "source": [
        "%%time \n",
        "\n",
        "x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values, tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 55s, sys: 2.01 s, total: 6min 57s\n",
            "Wall time: 6min 57s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9xRRFcM-aa-",
        "colab_type": "text"
      },
      "source": [
        "Датасеты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWI_R98o-aa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_train, y_train))\n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices((x_valid, y_valid))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .cache()\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(x_test)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB7SVWuz-abC",
        "colab_type": "text"
      },
      "source": [
        "Загрузка модели в ТПУ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvrMO2kf-abC",
        "colab_type": "code",
        "outputId": "f5338495-80fb-4c60-d236-478ec379227c",
        "colab": {
          "referenced_widgets": [
            "8dc4665cbcde4584b6f15fc63a7f4b8e"
          ]
        }
      },
      "source": [
        "%%time\n",
        "with strategy.scope():\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, loss='binary_crossentropy', max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8dc4665cbcde4584b6f15fc63a7f4b8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3271420488.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_word_ids (InputLayer)  [(None, 216)]             0         \n",
            "_________________________________________________________________\n",
            "tf_roberta_model (TFRobertaM ((None, 216, 1024), (None 559890432 \n",
            "_________________________________________________________________\n",
            "tf_op_layer_strided_slice (T [(None, 1024)]            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 559,891,457\n",
            "Trainable params: 559,891,457\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 2min 2s, sys: 42.9 s, total: 2min 44s\n",
            "Wall time: 2min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYq2nYcd-abF",
        "colab_type": "text"
      },
      "source": [
        "**Обучение**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1Gdg-1t-abF",
        "colab_type": "text"
      },
      "source": [
        "Сначала обучаемся на трейне (только английские комментарии), пробовалось различное число эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmN8q4lO-abG",
        "colab_type": "code",
        "outputId": "c20f4d30-a562-456b-8137-bbb034039745",
        "colab": {}
      },
      "source": [
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=n_steps,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 3404 steps, validate for 63 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n",
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3404/3404 [==============================] - 1957s 575ms/step - loss: 0.0665 - accuracy: 0.9738 - val_loss: 0.3910 - val_accuracy: 0.8719\n",
            "Epoch 2/2\n",
            "3404/3404 [==============================] - 1752s 515ms/step - loss: 0.0501 - accuracy: 0.9798 - val_loss: 0.3594 - val_accuracy: 0.8683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFDvz3kH-abJ",
        "colab_type": "text"
      },
      "source": [
        "Затем обучаемся еще на валидационной выборке (комментарии на разных языках), пробовалось различное число эпох"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFQ0dAT5-abJ",
        "colab_type": "code",
        "outputId": "c6a41b76-aeea-4a21-e66b-6cfa28ece121",
        "colab": {}
      },
      "source": [
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(\n",
        "    valid_dataset.repeat(),\n",
        "    steps_per_epoch=n_steps,\n",
        "    epochs=2\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 62 steps\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:430: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 69s 1s/step - loss: 0.2366 - accuracy: 0.8904\n",
            "Epoch 2/2\n",
            "62/62 [==============================] - 143s 2s/step - loss: 0.1511 - accuracy: 0.9369\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_pn_eTl-abM",
        "colab_type": "text"
      },
      "source": [
        "Предсказание и сабмит"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kLd0KQ1-abM",
        "colab_type": "code",
        "outputId": "d0f10ff2-16f7-46b6-d1fc-6489abd19d08",
        "colab": {}
      },
      "source": [
        "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "499/499 [==============================] - 115s 231ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY1CM0ir_gk2",
        "colab_type": "text"
      },
      "source": [
        "Public LB : 0.9365"
      ]
    }
  ]
}